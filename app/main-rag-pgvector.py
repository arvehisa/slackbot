import os
import re

from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

from langchain.chat_models import BedrockChat
from langchain.embeddings import BedrockEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.vectorstores.pgvector import PGVector

region="us-east-1"
model_id ="anthropic.claude-v2" #anthropic.claude-v2
llm = BedrockChat(model_id=model_id, region_name=region)  
# llm = ChatOpenAI(model_name=model_id) #gpt-4
embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")

CONNECTION_STRING = PGVector.connection_string_from_db_params(
    driver="psycopg2",
    host=os.environ.get("PGVECTOR_HOST"),
    port="5432",
    database="postgres",
    user="postgres",
    password=os.environ.get("PGVECTOR_PASSWORD"),
)

COLLECTION_NAME = "bedrock_documents"

vectorstore = PGVector(
    collection_name=COLLECTION_NAME,
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
)

prompt_template = """

Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
<context>
{context}
</context

Question: {question}

Assistant:"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT},
)

# Slackbot
SLACK_BOT_TOKEN = os.environ['SLACK_BOT_TOKEN']
SOCKET_MODE_TOKEN = os.environ['SOCKET_MODE_TOKEN']
SLACK_SIGNING_SECRET = os.environ['SLACK_SIGNING_SECRET']
app = App(token=SLACK_BOT_TOKEN)

@app.event("app_mention")
def mention(event, say):
    no_mention_text = re.sub(r'^<.*>', '', event['text'])
    chat_history = []
    """
    inputs = {
        "chat_history": chat_history,
        "question": no_mention_text,
    }
    """
    thread_ts = event['ts']
    result = qa({"query":no_mention_text})
    answer = result["result"]
    
    response = f""" {answer}\n\ngenerated by {model_id}"""
    
    say(text=response, thread_ts=thread_ts)

# socket mode の場合
# if __name__ == "__main__":
#     handler = SocketModeHandler(app, SOCKET_MODE_TOKEN)
#     handler.start()

if __name__ == "__main__":
    app.start(port=8080)