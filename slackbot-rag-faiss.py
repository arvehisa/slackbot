import os
import re
import pickle

from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

from langchain.llms import Bedrock
from langchain.embeddings import BedrockEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

#from langchain.chains import ConversationalRetrievalChain

model_id = os.environ['MODEL_ID']
llm = Bedrock(model_id=model_id)  #anthropic.claude-v2
# llm = ChatOpenAI(model_name=model_id) #gpt-4
embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v1")

# load text chunks
with open('docs.pkl', 'rb') as f:
    docs = pickle.load(f)

region=os.environ['AWS_REGION']

vectorstore_faiss = FAISS.load_local("faiss_index", embeddings) #ここ本当に合ってるかわからない
retriever = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)

prompt_template = """

Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
<context>
{context}
</context

Question: {question}

Assistant:"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa = RetrievalQA.from_chain_type(
    llm = llm,
    chain_type = "stuff",
    retriever = vectorstore_faiss.as_retriever(
        search_type = "similarity",
        search_kwargs={"k": 3}
        ),
    return_source_documents = True,
    chain_type_kwargs={"prompt": PROMPT}
)

# Slackbot
SLACK_BOT_TOKEN = os.environ['SLACK_BOT_TOKEN']
SOCKET_MODE_TOKEN = os.environ['SOCKET_MODE_TOKEN']
app = App(token=SLACK_BOT_TOKEN)

@app.event("app_mention")
def mention(event, say):
    no_mention_text = re.sub(r'^<.*>', '', event['text'])
    chat_history = []
    """
    inputs = {
        "chat_history": chat_history,
        "question": no_mention_text,
    }
    """
    thread_ts = event['ts']
    result = qa({"query":no_mention_text})
    answer = result["result"]
    
    response = f""" {answer}\n\ngenerated by {model_id}"""
    
    say(text=response, thread_ts=thread_ts)

# サーバーの起動
if __name__ == "__main__":
    handler = SocketModeHandler(app, SOCKET_MODE_TOKEN)
    handler.start()
